{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import h5py\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "#from keras.utils.visualize_util import to_graph\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import glob\n",
    "shuffle_data = True  # shuffle the addresses before saving\n",
    "hdf5_path = 'dataset.hdf5'  # address to where you want to save the hdf5 file\n",
    "\n",
    "\n",
    "#defining paths of the dataset and out tfrecord file\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "print(os.getcwd())\n",
    "\n",
    "#dirname = os.path.dirname(__file__)\n",
    "#data_path = os.path.join(dirname, '../../data_breast(copy)/*.png')\n",
    "data_path = '/vol/teaching/Teach_FYP/MediaAI/workspaces/2018/nc00494/data_breast(copy)'\n",
    "pathout = '/vol/teaching/Teach_FYP/MediaAI/workspaces/2018/nc00494/retina/TFrecords/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a list of datafile directories\n",
    "addrs =[]\n",
    "for path, subdirs, files in os.walk(data_path):\n",
    "    for name in files:\n",
    "        addrs.append(os.path.join(path, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the dimensions of input image just to be sure\n",
    "chkimg = cv2.imread(addrs[1])\n",
    "#cv2.imshow('wow',chkimg)\n",
    "\n",
    "plt.imshow(chkimg, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting a list of their corresponding lables\n",
    "labels = [0 if 'benign' in addr else 1 for addr in addrs]  # 0 = benign, 1 = malignant\n",
    "labels.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to shuffle data\n",
    "if shuffle_data:\n",
    "    c = list(zip(addrs, labels))\n",
    "    shuffle(c)\n",
    "    addrs, labels = zip(*c)\n",
    "    \n",
    "# Divide the hata into 80% train and 20% test\n",
    "train_addrs = addrs[0:int(0.8*len(addrs))]\n",
    "train_labels = labels[0:int(0.8*len(labels))]\n",
    "test_addrs = addrs[int(0.8*len(addrs)):]\n",
    "test_labels = labels[int(0.8*len(labels)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_order = 'tf'  # 'th' for Theano, 'tf' for Tensorflow\n",
    "# check the order of data and chose proper data shape to save images\n",
    "if data_order == 'th':\n",
    "    train_shape = (len(train_addrs), 3, 224, 224)\n",
    "    test_shape = (len(test_addrs), 3, 224, 224)\n",
    "elif data_order == 'tf':\n",
    "    train_shape = (len(train_addrs), 450, 700, 3)\n",
    "    test_shape = (len(test_addrs), 450, 700, 3)\n",
    "# open a hdf5 file and create earrays\n",
    "hdf5_file = h5py.File(hdf5_path, mode='w')\n",
    "hdf5_file.create_dataset(\"train_img\", train_shape, np.int8)\n",
    "hdf5_file.create_dataset(\"test_img\", test_shape, np.int8)\n",
    "hdf5_file.create_dataset(\"train_mean\", train_shape[1:], np.float32)\n",
    "hdf5_file.create_dataset(\"train_labels\", (len(train_addrs),), np.int8)\n",
    "hdf5_file[\"train_labels\"][...] = train_labels\n",
    "hdf5_file.create_dataset(\"test_labels\", (len(test_addrs),), np.int8)\n",
    "hdf5_file[\"test_labels\"][...] = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a numpy array to save the mean of the images\n",
    "mean = np.zeros(train_shape[1:], np.float32)\n",
    "# loop over train addresses\n",
    "for i in range(len(train_addrs)):\n",
    "    # print how many images are saved every 1000 images\n",
    "    if i % 1000 == 0 and i > 1:\n",
    "        print ('Train data: {}/{}'.format(i, len(train_addrs)))\n",
    "    # read an image and resize to (224, 224)\n",
    "    # cv2 load images as BGR, convert it to RGB\n",
    "    addr = train_addrs[i]\n",
    "    img = cv2.imread(addr)\n",
    "    img = cv2.resize(img, (700, 450), interpolation=cv2.INTER_AREA)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # add any image pre-processing here\n",
    "    # if the data order is Theano, axis orders should change\n",
    "    if data_order == 'th':\n",
    "        img = np.rollaxis(img, 2)\n",
    "    # save the image and calculate the mean so far\n",
    "    hdf5_file[\"train_img\"][i, ...] = img[None]\n",
    "    mean += img / float(len(train_labels))\n",
    "# loop over validation addresses\n",
    "# for i in range(len(val_addrs)):\n",
    "#     # print how many images are saved every 1000 images\n",
    "#     if i % 1000 == 0 and i > 1:\n",
    "#         print 'Validation data: {}/{}'.format(i, len(val_addrs))\n",
    "#     # read an image and resize to (224, 224)\n",
    "#     # cv2 load images as BGR, convert it to RGB\n",
    "#     addr = val_addrs[i]\n",
    "#     img = cv2.imread(addr)\n",
    "#     img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     # add any image pre-processing here\n",
    "#     # if the data order is Theano, axis orders should change\n",
    "#     if data_order == 'th':\n",
    "#         img = np.rollaxis(img, 2)\n",
    "#     # save the image\n",
    "#     hdf5_file[\"val_img\"][i, ...] = img[None]\n",
    "# loop over test addresses\n",
    "for i in range(len(test_addrs)):\n",
    "    # print how many images are saved every 1000 images\n",
    "    if i % 1000 == 0 and i > 1:\n",
    "        print('Test data: {}/{}'.format(i, len(test_addrs)))\n",
    "    # read an image and resize to (224, 224)\n",
    "    # cv2 load images as BGR, convert it to RGB\n",
    "    addr = test_addrs[i]\n",
    "    img = cv2.imread(addr)\n",
    "    img = cv2.resize(img, (700, 450), interpolation=cv2.INTER_AREA)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # add any image pre-processing here\n",
    "    # if the data order is Theano, axis orders should change\n",
    "    if data_order == 'th':\n",
    "        img = np.rollaxis(img, 2)\n",
    "    # save the image\n",
    "    hdf5_file[\"test_img\"][i, ...] = img[None]\n",
    "# save the mean and close the hdf5 file\n",
    "hdf5_file[\"train_mean\"][...] = mean\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdf5_path = 'dataset.hdf5'\n",
    "subtract_mean = False\n",
    "# open the hdf5 file\n",
    "hdf5_file = h5py.File(hdf5_path, \"r\")\n",
    "# subtract the training mean\n",
    "if subtract_mean:\n",
    "    mm = hdf5_file[\"train_mean\"][0, ...]\n",
    "    mm = mm[np.newaxis, ...]\n",
    "# Total number of samples\n",
    "data_num = hdf5_file[\"train_img\"].shape[0]\n",
    "data_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from math import ceil\n",
    "\n",
    "batch_size = 64\n",
    "nb_class=2\n",
    "# create list of batches to shuffle the data\n",
    "batches_list = list(range(int(ceil(float(data_num) / batch_size))))\n",
    "shuffle(batches_list)\n",
    "# loop over batches\n",
    "for n, i in enumerate(batches_list):\n",
    "    i_s = i * batch_size  # index of the first image in this batch\n",
    "    i_e = min([(i + 1) * batch_size, data_num])  # index of the last image in this batch\n",
    "    # read batch images and remove training mean\n",
    "    images = hdf5_file[\"train_img\"][i_s:i_e, ...]\n",
    "    if subtract_mean:\n",
    "        images -= mm\n",
    "    # read labels and convert to one hot encoding\n",
    "    labels = hdf5_file[\"train_labels\"][i_s:i_e]\n",
    "    labels_one_hot = np.zeros((batch_size, nb_class))\n",
    "    labels_one_hot[np.arange(batch_size), labels] = 1\n",
    "    print (n+1, '/', len(batches_list))\n",
    "    print (labels[0], labels_one_hot[0, :])\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(images[0])\n",
    "    plt.show()\n",
    "    if n == 5:  # break after 5 batches\n",
    "        break\n",
    "hdf5_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
